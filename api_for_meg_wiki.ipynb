{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, Path, Query\n",
        "from fastapi.testclient import TestClient\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class FandomWikiAPI:\n",
        "    def __init__(self, base_url=\"https://meg-endless-reality.fandom.com\"):\n",
        "        self.base_url = base_url\n",
        "        self.headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        }\n",
        "\n",
        "    def _make_request(self, url):\n",
        "        try:\n",
        "            response = requests.get(url, headers=self.headers, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            return response\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Ошибка запроса к {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_all_pages(self):\n",
        "        logger.info(\"Получение списка всех страниц\")\n",
        "        response = self._make_request(f\"{self.base_url}/wiki/Special:AllPages\")\n",
        "        if not response:\n",
        "            return []\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        pages = []\n",
        "\n",
        "        links = soup.find_all('a', href=True)\n",
        "        for link in links:\n",
        "            href = link['href']\n",
        "            if '/wiki/' in href and 'Special:' not in href and 'User:' not in href:\n",
        "                title = link.text.strip()\n",
        "                if title and title not in ['Wiki', 'Main Page', 'Home']:\n",
        "                    pages.append({\n",
        "                        'title': title,\n",
        "                        'url': href if href.startswith('http') else self.base_url + href\n",
        "                    })\n",
        "\n",
        "        logger.info(f\"Найдено {len(pages)} страниц\")\n",
        "        return pages[:20]\n",
        "\n",
        "    def get_page_content(self, page_url):\n",
        "        logger.info(f\"Получение содержимого страницы: {page_url}\")\n",
        "        if not page_url.startswith('http'):\n",
        "            page_url = self.base_url + page_url\n",
        "\n",
        "        response = self._make_request(page_url)\n",
        "        if not response:\n",
        "            return None\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        title = soup.find('h1')\n",
        "        if not title:\n",
        "            title = soup.find('title')\n",
        "        title_text = title.text.strip() if title else \"Неизвестно\"\n",
        "\n",
        "        content = soup.find('div', class_='mw-parser-output')\n",
        "        if not content:\n",
        "            content = soup.find('div', id='content')\n",
        "        if not content:\n",
        "            content = soup.find('main')\n",
        "        if not content:\n",
        "            content = soup.find('article')\n",
        "\n",
        "        if content:\n",
        "            for elem in content.find_all(['script', 'style']):\n",
        "                elem.decompose()\n",
        "\n",
        "            text_content = content.get_text(separator='\\n', strip=True)\n",
        "            text_content = ' '.join(text_content.split())\n",
        "        else:\n",
        "            text_content = \"Контент не найден\"\n",
        "\n",
        "        logger.info(f\"Получено содержимое страницы '{title_text}'\")\n",
        "        return {\n",
        "            'title': title_text,\n",
        "            'content': text_content,\n",
        "            'url': page_url\n",
        "        }\n",
        "\n",
        "app = FastAPI()\n",
        "wiki_api = FandomWikiAPI()\n",
        "\n",
        "@app.get(\"/pages\")\n",
        "def get_pages(limit: int = Query(10, description=\"Количество страниц для возврата\")):\n",
        "    logger.info(f\"Запрос на получение страниц с лимитом: {limit}\")\n",
        "    pages = wiki_api.get_all_pages()[:limit]\n",
        "    return {\n",
        "        \"total\": len(pages),\n",
        "        \"pages\": pages\n",
        "    }\n",
        "\n",
        "@app.get(\"/pages/{page_title}/content\")\n",
        "def get_page_content(\n",
        "    page_title: str = Path(..., description=\"Название страницы\", alias=\"page_title\")\n",
        "):\n",
        "    logger.info(f\"Запрос на получение содержимого страницы: {page_title}\")\n",
        "    page_url = f\"/wiki/{page_title}\"\n",
        "    content = wiki_api.get_page_content(page_url)\n",
        "\n",
        "    if not content:\n",
        "        logger.warning(f\"Страница '{page_title}' не найдена\")\n",
        "        return {\"error\": \"Страница не найдена\"}\n",
        "\n",
        "    return content\n",
        "\n",
        "@app.get(\"/search\")\n",
        "def search_pages(\n",
        "    query: str = Query(..., description=\"Поисковый запрос\"),\n",
        "    max_results: int = Query(5, description=\"Максимальное количество результатов\")\n",
        "):\n",
        "    logger.info(f\"Поиск страниц по запросу: '{query}', макс. результатов: {max_results}\")\n",
        "    all_pages = wiki_api.get_all_pages()\n",
        "\n",
        "    results = [\n",
        "        page for page in all_pages\n",
        "        if query.lower() in page['title'].lower()\n",
        "    ][:max_results]\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"found\": len(results),\n",
        "        \"results\": results\n",
        "    }\n",
        "\n",
        "#Тест\n",
        "client = TestClient(app)\n",
        "\n",
        "def test_api():\n",
        "    logger.info(\"Начало тестирования API\")\n",
        "\n",
        "    response1 = client.get(\"/pages?limit=3\")\n",
        "    logger.info(f\"Тест 1 - Статус: {response1.status_code}\")\n",
        "    assert response1.status_code == 200\n",
        "    data1 = response1.json()\n",
        "    print(f\"Получено страниц: {data1['total']}\")\n",
        "\n",
        "    if data1['total'] > 0:\n",
        "        first_page = data1['pages'][0]['title']\n",
        "        response2 = client.get(f\"/pages/{first_page}/content\")\n",
        "        logger.info(f\"Тест 2 - Статус: {response2.status_code}\")\n",
        "        assert response2.status_code == 200\n",
        "        data2 = response2.json()\n",
        "        print(f\"Содержимое страницы '{data2['title']}': {data2['content'][:100]}...\")\n",
        "\n",
        "    response = client.get(\"/pages/entities/content\")\n",
        "    logger.info(f\"Запрос через API - Статус: {response.status_code}\")\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        print(f\"Заголовок: {data['title']}\")\n",
        "        print(f\"URL: {data['url']}\")\n",
        "        print(f\"Содержимое страницы 'Entities': {data['content']}\")\n",
        "        print(\"=\" * 50)\n",
        "    else:\n",
        "        print(f\"Ошибка: {response.json()}\")\n",
        "\n",
        "    response3 = client.get(\"/search?query=wiki&max_results=2\")\n",
        "    logger.info(f\"Тест 3 - Статус: {response3.status_code}\")\n",
        "    assert response3.status_code == 200\n",
        "    data3 = response3.json()\n",
        "    print(f\"Найдено результатов по запросу '{data3['query']}': {data3['found']}\")\n",
        "\n",
        "    logger.info(\"Тестирование завершено успешно\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_api()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl6RR4mGmF8h",
        "outputId": "2a54112f-4ce9-4d56-954f-714f458ac831"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Получено страниц: 3\n",
            "Содержимое страницы 'Levels': Important Feature This content is important to game's core gameplay Reason: Levels are the main maps...\n",
            "Заголовок: Entities\n",
            "URL: https://meg-endless-reality.fandom.com/wiki/entities\n",
            "Содержимое страницы 'Entities': Important Feature This content is important to game's core gameplay Reason: Entities are the main antagonists of the game Entities are the main antagonists of the game with most levels having them. Most entities are relatively the same while others have unique mechanics that make them difficult to survive. Common Entities [ ] Entities that are able to spawn in most levels. Humans Bacteria Skin-Stealers Clumps Hounds Stranglers Facelings Phones Memory Worms The Virus Transporters Dullers Aranea Membri Special Entities [ ] Entities that can only spawn on special levels/objectives Smilers Death Moths Twins Partygoers Partypoopers Windows The Enraged Smiler The Neighborhood Watch The Thing on Level 7 Animations Uncle Samsonite Broogli Removed Entities [ ] Entities that have been removed for 1 or more reasons Standers Shrek Snowmen The Keeper\n",
            "==================================================\n",
            "Найдено результатов по запросу 'wiki': 0\n"
          ]
        }
      ]
    }
  ]
}